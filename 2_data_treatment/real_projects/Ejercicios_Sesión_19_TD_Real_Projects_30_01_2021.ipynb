{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicios SesiÃ³n 19 TD Real Projects 30-01-2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Jq9d0x1OTh2N"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark and Apache Kafka Library in VM\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dd1e21-c226-4f9d-8c07-c608044d30fb"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark==1.3.0\n",
        "!pip install py4j==0.10.9\n",
        "\n",
        "# For plotting\n",
        "!pip install folium\n",
        "!pip install plotly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from folium) (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from folium) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from folium) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from folium) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from folium) (2.11.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->folium) (1.1.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5sHazLNzqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d508d4-da14-4ab0-c14c-139c080d4922"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ngrok\t\t\t      spark-3.0.1-bin-hadoop2.7\n",
            "ngrok-stable-linux-amd64.zip  spark-3.0.1-bin-hadoop2.7.tgz\n",
            "sample_data\t\t      spark-3.0.1-bin-hadoop2.7.tgz.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_HtvSAj4sI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796abd82-6436-4bee-dc96-c8d6e982e828"
      },
      "source": [
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/libs/libs-kafka_301.zip --directory-prefix=/content/spark-3.0.1-bin-hadoop2.7/jars/\n",
        "!unzip -n /content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip -d /content/spark-3.0.1-bin-hadoop2.7/jars/\n",
        "!ls /content/spark-3.0.1-bin-hadoop2.7/jars/*kafka*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.4.1.jar\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip.1\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.12-3.0.1.jar\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/spark-token-provider-kafka-0-10_2.12-3.0.1.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qDLAxMTUYQ"
      },
      "source": [
        "Define the environment (Java & Spark homes)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSd4dfANNndH"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_IgZEv2XaDm"
      },
      "source": [
        "Starting Spark Session and print the version\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "51ef8992-8947-432d-87c1-3f336614e4c7"
      },
      "source": [
        "import findspark\n",
        "findspark.add_packages([\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1\"])\n",
        "findspark.add_jars([\"/content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.0.0.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/lz4-java-1.4.1-jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/scala-library-2.11.12.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/slf4j-api-1.7.25.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/unused-1.0.0.jar\"])\n",
        "findspark.init(\"spark-3.0.1-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.ui.port\", \"4050\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.0.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28MgeRJHKJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "d31034e4-df7e-4e74-ef64-1e18a3894172"
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b12d7ca84a87:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f8b595a3b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrOO29YRuDB"
      },
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating ngrok tunnel to allow Spark UI (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaab47c0-4353-4843-8422-b7034c4790bf"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-30 12:12:32--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 35.153.56.97, 34.202.112.21, 52.200.171.63, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|35.153.56.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ângrok-stable-linux-amd64.zip.1â\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  56.6MB/s    in 0.2s    \n",
            "\n",
            "2021-01-30 12:12:33 (56.6 MB/s) - ângrok-stable-linux-amd64.zip.1â saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://8dff1b1dfbd1.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruHocwYcT4aj"
      },
      "source": [
        "# Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musWXLzBUEQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d411138e-677b-441f-bb13-88f4e7ec81e3"
      },
      "source": [
        "!mkdir -p /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.json -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.edges.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.address.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.intermediary.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.officer.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.entity.csv -P /dataset\r\n",
        "!ls /dataset"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "offshore_leaks.edges.csv\t       offshore_leaks.nodes.intermediary.csv.1\n",
            "offshore_leaks.edges.csv.1\t       offshore_leaks.nodes.officer.csv\n",
            "offshore_leaks.nodes.address.csv       offshore_leaks.nodes.officer.csv.1\n",
            "offshore_leaks.nodes.address.csv.1     trades.csv\n",
            "offshore_leaks.nodes.entity.csv        trades.csv.1\n",
            "offshore_leaks.nodes.entity.csv.1      trades.json\n",
            "offshore_leaks.nodes.intermediary.csv  trades.json.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42PI1onm9kIh"
      },
      "source": [
        "# Project 1 - Regulatory Banking Project\r\n",
        "---\r\n",
        "\r\n",
        "Input files: /dataset/trades.csv & /dataset/trades.json\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebjNK66MgA_i"
      },
      "source": [
        "df_positions1 = spark \\\n",
        "  .read \\\n",
        "  .format(\"csv\") \\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .load(\"/dataset/trades.csv\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smNY3KF5oNMj",
        "outputId": "6a765b55-76e3-40ee-e28e-d8aec4f63715"
      },
      "source": [
        "df_positions1.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+--------+--------+------+---------+--------+-----+------------------+------------------+------------------+------------------+----------+\n",
            "|      Date|   Open|    High|     Low| Close|   Volume|Dividend|Split|          Adj_Open|          Adj_High|           Adj_Low|         Adj_Close|Adj_Volume|\n",
            "+----------+-------+--------+--------+------+---------+--------+-----+------------------+------------------+------------------+------------------+----------+\n",
            "|2017-12-28| 190.91|  190.98|  189.64|189.78|3175631.0|     0.0|  1.0| 177.5442795479532| 177.6093788071243|176.36319298870592|176.49339150704813| 3175631.0|\n",
            "|2017-12-27|  190.6|  191.49|  190.01|190.19|5912613.0|     0.0|  1.0|177.25598282876686| 178.0836734096567|176.70728907289606| 176.8746871679075| 5912613.0|\n",
            "|2017-12-26| 188.53|  190.42|  188.34|190.36|2969182.0|     0.0|  1.0|175.33090473613547|177.08858473375543|175.15420674695673|177.03278536875163| 2969182.0|\n",
            "|2017-12-22|  188.2|  188.46|  187.27|188.13|3256519.0|     0.0|  1.0| 175.0240082286145|175.26580547696435|174.15911807105547| 174.9589089694434| 3256519.0|\n",
            "|2017-12-21|  187.7|  188.84|  187.44|188.08|5859058.0|     0.0|  1.0| 174.5590135202494|175.61920145532181|174.31721627189958| 174.9124094986069| 5859058.0|\n",
            "|2017-12-20| 187.14|187.9024| 186.035|187.31|5383672.0|     0.0|  1.0|174.03821944688053| 174.7472433781956|173.01058114139371|174.19631764772467| 5383672.0|\n",
            "|2017-12-19|185.235|  186.72|   184.6|185.98|4496706.0|     0.0|  1.0| 172.2665896080096|173.64762389185387|171.67604632838595|172.95943172347356| 4496706.0|\n",
            "|2017-12-18| 183.96|  185.47|   183.5|184.73|5011827.0|     0.0|  1.0|171.08085310167866|172.48513712094118| 170.6530579699828|171.79694495256086| 5011827.0|\n",
            "|2017-12-15|183.005|  183.21|  182.13|182.58|8201699.0|     0.0|  1.0|170.19271320870135|170.38336103913105| 169.3789724690625|169.79746770659105| 8201699.0|\n",
            "|2017-12-14| 183.88|   184.0|  182.06|182.13|3542677.0|     0.0|  1.0|171.00645394834024|171.11805267834788|169.31387320989137| 169.3789724690625| 3542677.0|\n",
            "|2017-12-13| 182.01|  183.67|   182.0|183.03|5176749.0|     0.0|  1.0|169.26737373905485| 170.8111561708269|169.25807384488758| 170.2159629441196| 5176749.0|\n",
            "|2017-12-12| 182.75|  183.21|  181.66| 181.8|5274935.0|     0.0|  1.0| 169.9555659074352|170.38336103913105| 168.9418774431993|169.07207596154154| 5274935.0|\n",
            "|2017-12-11|  182.9|  182.96|181.1201|182.25|6038891.0|     0.0|  1.0| 170.0950643199447|170.15086368494852| 168.4397761571067| 169.4905711990701| 6038891.0|\n",
            "|2017-12-08|  182.5|   183.9|  182.16|183.41|5091538.0|     0.0|  1.0|169.72306855325263|171.02505373667486| 169.4068721515644|170.56935892247708| 5091538.0|\n",
            "|2017-12-07| 180.05|  182.58|  179.77| 182.0|5484611.0|     0.0|  1.0|167.44459448226377|169.79746770659105|167.18419744557934|169.25807384488758| 5484611.0|\n",
            "|2017-12-06| 180.25|  182.15|  178.68| 180.8|6891093.0|     0.0|  1.0| 167.6305923656098| 169.3975722573971|166.17050898134346| 168.1420865448114| 6891093.0|\n",
            "|2017-12-05| 184.79|  184.91|182.2589|182.85|6347228.0|     0.0|  1.0|171.85274431756469| 171.9643430475723|169.49884810487902|170.04856484910817| 6347228.0|\n",
            "|2017-12-04| 183.19|  186.31|  183.19| 184.9|6192507.0|     0.0|  1.0|170.36476125079645|173.26632823099453|170.36476125079645|  171.955043153405| 6192507.0|\n",
            "|2017-12-01| 180.32|   180.6|   176.7|180.42|4682482.0|     0.0|  1.0| 167.6956916247809|167.95608866146534|164.32912993621775| 167.7886905664539| 4682482.0|\n",
            "|2017-11-30| 178.07|  180.67|  177.41|179.82|9179321.0|     0.0|  1.0|165.60321543713806|168.02118792063646|164.98942242209617|167.23069691641584| 9179321.0|\n",
            "+----------+-------+--------+--------+------+---------+--------+-----+------------------+------------------+------------------+------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpijEdOnvCIQ",
        "outputId": "9efe5f8f-7812-4697-8f76-ec0a632afee0"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "def PnL(open, close): \n",
        "  if (open is not None and close is not None):\n",
        "    return ((close - open) / open * 100 )\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "spark.udf.register(\"pnl_udf\", PnL)\n",
        "# OR pnl_udf=udf(pnl)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.PnL>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRtKdnhLw9ZV",
        "outputId": "619e43a6-f173-403e-937a-6ae8ebaa827b"
      },
      "source": [
        "df_positions1_PnL = \\\n",
        "  df_positions1 \\\n",
        "    .withColumn(\"PnL\", round(PnL(col(\"Open\"), col(\"Close\")), 2)) \\\n",
        "    .withColumn(\"Origin\", lit(\"trades.csv\")) \\\n",
        "    .select(\"Date\", \"Open\", \"Close\", \"PnL\", \"Origin\") \\\n",
        "    .show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+------+-----+----------+\n",
            "|      Date|   Open| Close|  PnL|    Origin|\n",
            "+----------+-------+------+-----+----------+\n",
            "|2017-12-28| 190.91|189.78|-0.59|trades.csv|\n",
            "|2017-12-27|  190.6|190.19|-0.22|trades.csv|\n",
            "|2017-12-26| 188.53|190.36| 0.97|trades.csv|\n",
            "|2017-12-22|  188.2|188.13|-0.04|trades.csv|\n",
            "|2017-12-21|  187.7|188.08|  0.2|trades.csv|\n",
            "|2017-12-20| 187.14|187.31| 0.09|trades.csv|\n",
            "|2017-12-19|185.235|185.98|  0.4|trades.csv|\n",
            "|2017-12-18| 183.96|184.73| 0.42|trades.csv|\n",
            "|2017-12-15|183.005|182.58|-0.23|trades.csv|\n",
            "|2017-12-14| 183.88|182.13|-0.95|trades.csv|\n",
            "|2017-12-13| 182.01|183.03| 0.56|trades.csv|\n",
            "|2017-12-12| 182.75| 181.8|-0.52|trades.csv|\n",
            "|2017-12-11|  182.9|182.25|-0.36|trades.csv|\n",
            "|2017-12-08|  182.5|183.41|  0.5|trades.csv|\n",
            "|2017-12-07| 180.05| 182.0| 1.08|trades.csv|\n",
            "|2017-12-06| 180.25| 180.8| 0.31|trades.csv|\n",
            "|2017-12-05| 184.79|182.85|-1.05|trades.csv|\n",
            "|2017-12-04| 183.19| 184.9| 0.93|trades.csv|\n",
            "|2017-12-01| 180.32|180.42| 0.06|trades.csv|\n",
            "|2017-11-30| 178.07|179.82| 0.98|trades.csv|\n",
            "+----------+-------+------+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h6LICd9ZBPw"
      },
      "source": [
        "# Project 2 - Transactions Notifications\r\n",
        "\r\n",
        "*Hint: https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49tIaJ7gZN2C"
      },
      "source": [
        "from pyspark.sql.functions import from_json, col\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType\r\n",
        "\r\n",
        "df = spark \\\r\n",
        "  .readStream \\\r\n",
        "  .format(\"kafka\") \\\r\n",
        "  .option(\"kafka.bootstrap.servers\", \"ec2-34-232-50-77.compute-1.amazonaws.com:9092\") \\\r\n",
        "  .option(\"subscribe\", \"transactions\") \\\r\n",
        "  .load()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6SLWzYRZbud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e041ab61-cfb0-4f3f-d9bd-2589a9a2f6f8"
      },
      "source": [
        "schema = StructType(\r\n",
        "    [\r\n",
        "     StructField('Account No', StringType(), True),\r\n",
        "     StructField('DATE', StringType(), True),\r\n",
        "     StructField('TRANSACTION DETAILS', StringType(), True),\r\n",
        "     StructField('CHQ.NO.', StringType(), True),\r\n",
        "     StructField('VALUE DATE', StringType(), True),\r\n",
        "     StructField(' WITHDRAWAL AMT ', StringType(), True),\r\n",
        "     StructField(' DEPOSIT AMT ', StringType(), True),\r\n",
        "     StructField('BALANCE AMT', StringType(), True)\r\n",
        "    ]\r\n",
        ")\r\n",
        "df.printSchema()\r\n",
        "\r\n",
        "dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\r\n",
        "    .withColumn(\"value\", from_json(\"value\", schema)) \\\r\n",
        "    .select(col('key'), col(\"timestamp\"), col('value.*'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rfuD70_O6ZN"
      },
      "source": [
        "dataset_count = dataset \\\n",
        "  .groupBy(window(\"timestamp\", \"1 minute\")) \\\n",
        "  .count()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh82Gqz7Z1bm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebddc44-beb8-48da-a54d-230597614cc3"
      },
      "source": [
        "dataset_count.writeStream \\\r\n",
        " .outputMode(\"update\") \\\r\n",
        " .format(\"memory\") \\\r\n",
        " .option(\"truncate\", \"false\") \\\r\n",
        " .queryName(\"transactions_count\") \\\r\n",
        " .start()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.StreamingQuery at 0x7f8b58ccfe10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2KkvViQaXKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51944a65-d32f-4588-ad93-cfc6118024b9"
      },
      "source": [
        "spark.sql(\"select * from transactions_count\").show(truncate = False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|[2021-01-30 12:20:00, 2021-01-30 12:21:00]|150  |\n",
            "|[2021-01-30 12:20:00, 2021-01-30 12:21:00]|350  |\n",
            "|[2021-01-30 12:20:00, 2021-01-30 12:21:00]|500  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|100  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|200  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|300  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|400  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|550  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|50   |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|600  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|150  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|250  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|350  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|450  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|600  |\n",
            "+------------------------------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG8_q4f5Q_E6"
      },
      "source": [
        "# Project 3 - Panama Papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NApu7049VPwm"
      },
      "source": [
        "Trace \"Spring Song International Co., Ltd.\" entity with Spark SQL using the following dataset</br>\r\n",
        "/dataset/offshore_leaks.nodes.entity.csv </br>\r\n",
        "/dataset/offshore_leaks.nodes.intermediary.csv </br>\r\n",
        "/dataset/offshore_leaks.edges.csv </br>\r\n",
        "/dataset/offshore_leaks.nodes.officer.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ydUZNSrYGyN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}