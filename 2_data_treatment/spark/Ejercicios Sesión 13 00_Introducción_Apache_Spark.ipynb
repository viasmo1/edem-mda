{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00.Introducción_Apache_Spark_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yzU_4EjAjZgh"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark and Apache Kafka Library in VM\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z"
      },
      "source": [
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark3.0.1\n",
        "!wget -q https://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip it\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install py4j\n",
        "\n",
        "# For maps\n",
        "!pip install folium\n",
        "!pip install plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qDLAxMTUYQ"
      },
      "source": [
        "Define the environment (Java & Spark homes)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf95Ld34SX4j"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSd4dfANNndH"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdFv-xxITa2J"
      },
      "source": [
        "Starting Spark Session and print the version\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.0.1-bin-hadoop3.2\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.ui.port\", \"4500\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28MgeRJHKJ5"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrOO29YRuDB"
      },
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating ngrok tunnel to allow Spark UI (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4500 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z0h3dF9Vg4X"
      },
      "source": [
        "# Descargar Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDin-0sXgyI"
      },
      "source": [
        "!mkdir -p /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/frankenstein.txt -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/el_quijote.txt -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/characters.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/planets.csv -P /dataset\n",
        "!ls /dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Zwm3NRXS_I"
      },
      "source": [
        "# RDD\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o6f6QOjTcZ"
      },
      "source": [
        "## Ejemplo 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnbafeFCVk8d"
      },
      "source": [
        "textFile = spark.sparkContext.textFile(\"/dataset/frankenstein.txt\")\n",
        "textFile.first()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHDroSQ1ZQSB"
      },
      "source": [
        "Abre la Spark UI e investiga qué ha sucedido\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a00GmwOZmM2"
      },
      "source": [
        "\n",
        "## Creación de colecciones paralelizadas\n",
        "Una manera muy rápida de crear RDD desde la shell, cuando estamos aprendiendo, es crear una colección paralelizada. Para ello:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzU_4EjAjZgh"
      },
      "source": [
        "## Ejemplo 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgireGq6YWEj"
      },
      "source": [
        "distData = spark.sparkContext.parallelize([25, 20, 15, 10, 5])\n",
        "distData.reduce(lambda x ,y: x + y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5hVQSRDaNAZ"
      },
      "source": [
        "¿De qué tipo es la variable distData?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0FXU7JawRm"
      },
      "source": [
        "## Ejercicio 1\n",
        "Cuenta el número de líneas del fichero \"el_quijote.txt\"\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCDo_-PiaEVl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prvVhMD4a5o7"
      },
      "source": [
        "## Ejercicio 2\n",
        "Imprime la primera línea del fichero \"el_quijote.txt\"\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vgL2Upsa-Qg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAxxBSrBb92y"
      },
      "source": [
        "## Transformaciones y Acciones sobre RDDs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-jBGb_acVuZ"
      },
      "source": [
        "### Acciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fc-rQBNjnNi"
      },
      "source": [
        "### Ejemplo 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvxep4yubxtC"
      },
      "source": [
        "print(textFile.count()) # Número de elementos en el RDD\n",
        "print(textFile.first()) # Primer elemento del RDD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYhM504ycl9K"
      },
      "source": [
        "### Transformaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irxzzmfwjyYi"
      },
      "source": [
        "### Ejemplo 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpkUk7t9cfoL"
      },
      "source": [
        "# ReduceByKey\n",
        "lines = spark.sparkContext.textFile(\"/dataset/frankenstein.txt\")\n",
        "pairs = lines.map(lambda s: (s, 1))\n",
        "counts = pairs.reduceByKey(lambda a, b: a + b).cache()\n",
        "counts.count()\n",
        "counts.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfajHIRsejog"
      },
      "source": [
        "# SortByKey\n",
        "sorted = counts.sortByKey()\n",
        "sorted.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFIlrClreyzI"
      },
      "source": [
        "Abre la Spark UI e investiga que ha ido pasando."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6c2qVSLj4Cy"
      },
      "source": [
        "### Ejemplo 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDwoLMbbdGPN"
      },
      "source": [
        "# Filter\n",
        "\n",
        "linesWithSpark = textFile.filter(lambda line: \"the\" in line)\n",
        "linesWithSpark.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngS6b5jUfYen"
      },
      "source": [
        "### Ejercicio 3\n",
        "Obtén el número de ocurrencias de cada palabra del fichero \"frankenstein.txt\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqbwlZn8fVNd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034ZWkexhXQF"
      },
      "source": [
        "### Ejercicio 4\n",
        "Obtén el top-10 de palabras con más de 4 caracteres\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QjYLZ0MgJ1_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IqqLY6PRtGg"
      },
      "source": [
        "## Key/Value Pair RDD\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq6NhwCXRl7n"
      },
      "source": [
        "### Ejemplo 6\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R04SZu82R1ui"
      },
      "source": [
        "charac_sw = spark.sparkContext.textFile(\"/dataset/characters.csv\")\n",
        "planets_sw = spark.sparkContext.textFile(\"/dataset/planets.csv\")\n",
        "charac_sw.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fuMXUgnS_Rb"
      },
      "source": [
        "planets_sw.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbYKGuvxPiqb"
      },
      "source": [
        "from itertools import islice\n",
        "\n",
        "charac_sw_noheader = charac_sw.mapPartitionsWithIndex(\n",
        "    lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
        "\n",
        "planets_sw_noheader = planets_sw.mapPartitionsWithIndex(\n",
        "    lambda idx, it: islice(it, 1, None) if idx == 0 else it)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1BEEwqpTREy"
      },
      "source": [
        "### Ejercicio 5\n",
        "Obtén un listado con la población del planeta al que pertenece cada personaje de Star Wars\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo0dq-QrU0MU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}